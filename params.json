{"google":"","note":"Don't delete this file! It's used internally to help with page regeneration.","name":"Accumulate","tagline":"This module allows for simulation and analysis of 2 choice accumulated category experimental designs.","body":"This module allows for simulation and analysis of 2 choice accumulated category experimental designs.\r\n\r\n# Preliminaries\r\n\r\nWe assume there are two categories, A and B and that there are some (unknown and irrelevant, from the perspective of this model) number of exemplars for each category.  We further assume that the exemplars are perfectly identified as A/B by each participant and that each A and B is equally weighted.  These assumptions allow for a full exploration by every subject of the entire A/B space (perhaps in replicate) for trial lengths of l = 10 (l must be even).  For example, if *l* = 8 there are 256 trials.  If every trial takes 9 seconds, then the whole series can be explored in 38 minutes.\r\n\r\nBy using all trials we can explore the complete consequences of many different category decision models (see 'Decision models').  We can also isolate the most, or least successful models, of most or least difficult trials for a given strategy(ies) or for each trial holistically (see 'Trial measures'), or even find those key trials where two largely similar models may diverge.\r\n\r\nSimulations are easy.  Subjects are hard.\r\n\r\n# A simple use case (run inside ipython):\r\n\r\nSo bring in the module...\r\n\r\n\timport accumulate\r\n\r\nNow we can do some work...\r\n\r\nLet us consider trials with a max length of 4.  This is shorter than you'll want in practice but allows for easy visualization of the results.  Also, to simplify implementation *l* must be even.\r\n\r\nFirst create an exhaustive experiment.  AccumulationTrials() takes one argument, the max trial length, *l*.\r\n\t\r\n\tl = 4\r\n\texp1 = accumulate.sim.base.Trials(l)\r\n\t\t\r\nSo what do we have?  All possible trial designs live in trials.\r\n\r\n\texp1.trials\r\n\r\nIs....\r\n\r\n\t[('A', 'A', 'A', 'A'),\r\n\t ('A', 'A', 'A', 'B'),\r\n\t ('A', 'A', 'B', 'A'),\r\n\t ('A', 'A', 'B', 'B'),\r\n\t ('A', 'B', 'A', 'A'),\r\n\t ('A', 'B', 'A', 'B'),\r\n\t ('A', 'B', 'B', 'A'),\r\n\t ('A', 'B', 'B', 'B'),\r\n\t ('B', 'A`', 'A', 'A'),\r\n\t ('B', 'A', 'A', 'B'),\r\n\t ('B', 'A', 'B', 'A'),\r\n\t ('B', 'A', 'B', 'B'),\r\n\t ('B', 'B', 'A', 'A'),\r\n\t ('B', 'B', 'A', 'B'),\r\n\t ('B', 'B', 'B', 'A'),\r\n\t ('B', 'B', 'B', 'B')]\r\n\r\nSo first we will assume that the participant makes decisions by counting the number of category A and B exemplars, and if that count exceeds some threshold (modeled here as a fraction of *l*) then makes a decision.  \r\n\r\nFirst assume they're in hurry and only wait for a 60% threshold.\r\n\r\n\td_fast = exp1.categorize(decide='count',threshold=0.6)\r\n\r\nd_fast is:\r\n\r\n\t[('A', 0.75, 0.0, 3),\r\n\t ('A', 0.75, 0.0, 3),\r\n\t ('A', 0.75, 0.25, 4),\r\n\t ('N', -1, -1, -1),\r\n\t ('A', 0.75, 0.25, 4),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('B', 0.75, 0.25, 4),\r\n\t ('A', 0.75, 0.25, 4),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('B', 0.75, 0.25, 4),\r\n\t ('N', -1, -1, -1),\r\n\t ('B', 0.75, 0.25, 4),\r\n\t ('B', 0.75, 0.0, 3),\r\n\t ('B', 0.75, 0.0, 3)]\r\n\r\nEach set of parenthesis is \r\n\t\r\n\t(decision, score for that decision, score for the unchosen option, how many exemplars were experienced until decision was made)\r\n\r\n'N' trials had no decision (-1 in these results means None or NA or 'Does not apply').\r\n\r\nNow let us compare that to a more patient ideal participant (who is coincidentally wanting statistical significance).\r\n\r\n\td_slow = exp1.categorize(decide='count',threshold=0.95)\r\n\r\nd_slow is:\r\n\r\n\t[('A', 1.0, 0.0, 4),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('N', -1, -1, -1),\r\n\t ('B', 1.0, 0.0, 4)]\r\n\r\n Being so careful they only could reach a decision on the 2 completely A and B trials....\r\n\r\n \r\n# Decision models\r\n\r\n The other decision models are 'likelihood', 'bayes', 'drift', 'last', 'first', and 'drift'.\r\n\r\n 1. 'count' see above\r\n\r\n 2. 'likelihood':  you can think of this as the scantron strategy.  In it we assume the Participants is mostly sensitive to local probability changes. It calculates the probability of the number of As (or Bs) in row.  When the probability exceeds threshold, a decision is made. For example:\r\n\r\n *p* begins at 0.5.  Assume we had an A to start. If exemplar 2 is A, *p* becomes 0.25.  If exemplar 2 is A, *p* is 0.12.  So if the threshold was 0.7 it would the subject would have decided on exemplar 2, and it is was 0.8, 3 would have done it. And so on....\r\n\r\n 3. 'bayes' will implement an Bayesian estimates for A/B.\r\n\r\n 4. 'drift' will implement a version of Ratcliffe's drift diffusion model\r\n\r\n 5. 'last' is the idiot's guess.  It models the case where the Participants waits till the end of the trial the guesses whatever the last exemplar was.\r\n\r\n 6. 'first' is the opposite of last.\r\n\r\n 7. 'information' calculates the rolling binomial entropy for A and B, and uses this, with threshold, to make a decision.\r\n\r\n 8. More to come....\r\n\r\n# Trial measures.\r\n\r\nTo get the total (A,B) counts for all trials do:\r\n\r\n\texp1.counts()\r\n\r\nWhich for l = 4 is:\r\n\r\n\t[(4, 0),\r\n\t (3, 1),\r\n\t (3, 1),\r\n\t (2, 2),\r\n\t (3, 1),\r\n\t (2, 2),\r\n\t (2, 2),\r\n\t (1, 3),\r\n\t (3, 1),\r\n\t (2, 2),\r\n\t (2, 2),\r\n\t (1, 3),\r\n\t (2, 2),\r\n\t (1, 3),\r\n\t (1, 3),\r\n\t (0, 4)]\r\n\r\nFinally in on order to asses overall trial difficulty I implemented a function that returns the minimum Hamming Distance (http://en.wikipedia.org/wiki/Hamming_distance) between each trial and the two 'undecidable' trials.\r\n\r\n'Undecidable' trials for l = 4 are A,B,A,B and B,A,B,A.\r\n\r\nThis distance measure allows you to quantify the objective difficulty of the whole trial, whereas the length until reaching decision criterion gives a 'local' (intra-trial) difficulty estimate.\r\n\r\nP.S. 'Undecidable' isn't true for all strategies (e.g. 'last' above), but is the most difficult possible for all strategies.  It is an imperfect term but still I think a good reference.\r\n\r\nDistance by example (l=4).\r\n\r\n\texp1.distances()\r\n\r\nGave\r\n\r\n\t[2, 1, 1, 2, 1, 0, 2, 1, 1, 2, 0, 1, 2, 1, 1, 2]\r\n\r\nSmaller numbers mean harder trials; smaller means closer to undecidable.\r\n\r\n# Statistics\r\n\r\nThe accumulate.stats submodule calculates aggregate statistics for AccumulationTrials.categorize() results.  Right now it only implements a scores function that returns (M,SD,VAR,N) for A,B and N for score (the value of the selected), off_score (the values for the unchosen option) and length (until criterion).\r\n\r\nFor example:\r\n\r\n\tresult1 = exp1.categorize('likelihood',0.7)\r\n\tstats1 = accumulate.stats.scores(result1)\r\n\r\nTo see the stats\r\n\r\n\tstats1.items()\r\n\r\n\t[('A',\r\n\t  {'length': (2.5714285714285716, 0.53061224489795922, 0.72843135908468359, 7),\r\n\t   'off_score': (0.5, 0.0, 0.0, 7),\r\n\t   'score': (0.75, 0.0, 0.0, 7)}),\r\n\t ('B',\r\n\t  {'length': (2.5714285714285716, 0.53061224489795922, 0.72843135908468359, 7),\r\n\t   'off_score': (0.5, 0.0, 0.0, 7),\r\n\t   'score': (0.75, 0.0, 0.0, 7)}),\r\n\t ('N',\r\n\t  {'length': (-1.0, 0.0, 0.0, 2),\r\n\t   'off_score': (-1.0, 0.0, 0.0, 2),\r\n\t   'score': (-1.0, 0.0, 0.0, 2)})]\r\n\r\n"}